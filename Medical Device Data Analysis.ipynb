{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Work in Progress ***\n",
    "\n",
    "Pubmed Query Builder: https://pubmed.ncbi.nlm.nih.gov/advanced/\n",
    "\n",
    "### Notes\n",
    "* Dates stored in __repr__ format in JSON\n",
    "* 000 = Code for unobtainable PMUID\n",
    "\n",
    "### MeSH Terms\n",
    "* Biomedical Engineering \n",
    "* Biomedical Technology\n",
    "* Equipment Safety\n",
    "* Equipment Design\n",
    "* Prostheses and Implants\n",
    "\n",
    "### Querys\n",
    "((\"Equipment and Supplies\"[MeSH Terms] OR \"Equipment Design\"[MeSH Terms] OR \"Equipment Safety\"[MeSH Terms] OR \"Biomedical Technology\"[MeSH Terms] OR \"Biomedical Engineering\"[MeSH Terms]) AND 1850/01/01:1997/12/31[Date - Publication]) AND (English[Language])\n",
    "\n",
    "(((medical device[Title/Abstract]) OR (medical devices[Title/Abstract])) AND (english[Language])) AND ((\"1850\"[Date - Publication] : \"2003\"[Date - Publication]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO:\n",
    "Create relative word frequencies function'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-5Jn7QOhBFO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter  \n",
    "from matplotlib import pyplot as plt\n",
    "from pymed import PubMed\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Pymed to get results for the desired query\n",
    "def search(query_string, pubmed_module, max_results=2000):\n",
    "    results = pubmed_module.query(query_string, max_results=max_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets relevant data from Pymed iterable and creates a dictionary\n",
    "def pubmed_todict(pubmed_iterable):\n",
    "    new_dict = {\"Result Number\": [], \"Pubmed ID\": [], \"Title\": [], \"Journal\": [], \"Authors\": [], \"Date\": []}\n",
    "    \n",
    "    # Assemble dictionary\n",
    "    for i, article in enumerate(pubmed_iterable):\n",
    "        new_dict[\"Result Number\"].append(i)\n",
    "        if \"\\n\" in article.pubmed_id:  # Deals with parsing issue for article PMUID\n",
    "            new_dict[\"Pubmed ID\"].append(\"000\")\n",
    "        else:\n",
    "            new_dict[\"Pubmed ID\"].append(article.pubmed_id)   \n",
    "        new_dict[\"Title\"].append(article.title)\n",
    "        try:  # Deals with books and book reviews (rather than articles) in the database\n",
    "            new_dict[\"Journal\"].append(article.journal)\n",
    "        except AttributeError:\n",
    "            new_dict[\"Journal\"].append(\"Book\") \n",
    "        new_dict[\"Authors\"].append(article.authors)\n",
    "        if type(article.publication_date) == datetime.date:  # Standardizes data - pymed returns some years as int\n",
    "            new_dict[\"Date\"].append(article.publication_date) \n",
    "        else:\n",
    "            new_dict[\"Date\"].append(datetime.datetime.strptime(str(article.publication_date), \"%Y\"))\n",
    "        \n",
    "    # Cleans author data\n",
    "    clean_author_list_by_paper = []\n",
    "    for paper_author_list in new_dict[\"Authors\"]:\n",
    "        if paper_author_list == []:\n",
    "            clean_author_list_by_paper.append([{\"name\": None, \"affiliation\": None}])\n",
    "        else:\n",
    "            author_list_by_paper = []\n",
    "            for author in paper_author_list:\n",
    "                if author[\"lastname\"] != None or author[\"initials\"] != None:\n",
    "                    try:\n",
    "                        author_dict = {\"name\": \"{} {}\".format(author[\"lastname\"], author[\"initials\"]),\n",
    "                                \"affiliation\": author[\"affiliation\"]}\n",
    "                    except KeyError:\n",
    "                        author_dict = {\"name\": \"{} {}\".format(author[\"lastname\"], author[\"initials\"]),\n",
    "                                    \"affiliation\": None}\n",
    "                    author_list_by_paper.append(author_dict)\n",
    "            clean_author_list_by_paper.append(author_list_by_paper)\n",
    "    \n",
    "    new_dict[\"Authors\"] = clean_author_list_by_paper\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoids serialization error with datetime in JSON dump\n",
    "# See https://stackoverflow.com/questions/54557568/typeerror-object-of-type-date-is-not-json-serializable\n",
    "def datetime_converter(object):\n",
    "    if isinstance(object, datetime.date):\n",
    "        return object.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_dump(query_string, max_results, file_name):\n",
    "    pubmed = PubMed(tool=\"Medical Device Author Network Analysis\", email=\"bowrey@umd.edu\")\n",
    "    results = search(query_string=query_string, pubmed_module=pubmed, max_results=max_results)\n",
    "    new_dict = pubmed_todict(pubmed_iterable=results)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        # Default argument takes function that is called when JSON ecounters object it can't convert\n",
    "        json.dump(new_dict, f, indent=4, default=datetime_converter)\n",
    "        print(\"Search Completed. File dumped to JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Completed. File dumped to JSON\n"
     ]
    }
   ],
   "source": [
    "# Searches for all items with \"medical device(s)\" in the title or abstract\n",
    "search_and_dump(query_string='''(((medical device[Title/Abstract]) OR (medical devices[Title/Abstract])) AND (english[Language])) \n",
    "                                AND ((\"1850\"[Date - Publication] : \"2004\"[Date - Publication]))''',\n",
    "               max_results=3000, file_name=\"device_pubmed_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some articles are listed with abnormal years (e.g. years outside of the range that was input to the API). I believe this is due to the Pymed module pulling the date of an article cited by the article in question rather than the date for the article itself. These errors are not randomly distributed. They overwhelmingly happen in the final year of the search range. My current solution is to extend the search range beyond what I want (up to the year 2000), then discarding all values outside of that range. This should pick up most of the incorrectly labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file_path, cutoff_year=2000):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        full_dict = json.load(f)\n",
    "    \n",
    "    slim_dict = {key:value for (key, value) in full_dict.items() if key in [\"Pubmed ID\", \"Title\", \"Journal\", \"Date\"]}\n",
    "    df = pd.DataFrame.from_dict(slim_dict)\n",
    "    # df = df[df[\"Pubmed ID\"] != \"000\"]  # Removes data with parsing error\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x : eval(x).year)  # Converts date from __repr__ format\n",
    "    \n",
    "    # Cleaning years\n",
    "    for index in df.loc[df[\"Date\"] > cutoff_year].index:\n",
    "        df.drop([index], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_df = create_df(\"device_pubmed_data.json\", 2000)\n",
    "device_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publications by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_date_group = device_df.groupby([\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_pub_year_table = device_date_group[\"Journal\"].apply(lambda y: y.agg(lambda x : x.value_counts().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 41 artists>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(device_pub_year_table.index, [device_pub_year_table.get(x) for x in device_pub_year_table.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Export to Voyant Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voyant_export(year_group, dataset_name, start_year, end_year=2000):\n",
    "    year_title_list = year_group[\"Title\"].unique()\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        temp_list = []\n",
    "        try:\n",
    "            for title in year_title_list[year]:\n",
    "                title = title.replace(\"--\", \" \")\n",
    "                title = title + \"\\n\"\n",
    "                temp_list.append(title)\n",
    "            \n",
    "            with open(\"Title Data Files/med_dev/{}_{}.txt\".format(year, dataset_name), \"w\") as f:\n",
    "                f.writelines(temp_list)\n",
    "       \n",
    "        except KeyError:  # If there were no publication in a year in the range\n",
    "            pass\n",
    "    \n",
    "    print(\"Data Exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical Device Publication Word Frequency: https://voyant-tools.org/?corpus=fa33b04ff82a6bb8eb2bb7c2b5c9837a&panels=corpusterms,termsberry,trends,summary,contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Exported\n"
     ]
    }
   ],
   "source": [
    "voyant_export(year_group = device_date_group, dataset_name=\"med_dev\", start_year=1952, end_year=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_info(pubmed_data, write_file):\n",
    "    identifier_list = {}\n",
    "    id_count = 1\n",
    "    \n",
    "    # Create data structure for author\n",
    "    for i, paper in enumerate(pubmed_data[\"Authors\"]):\n",
    "        for author in paper:\n",
    "            if author[\"name\"] != None:\n",
    "                \n",
    "                # If author is already in the dictionary\n",
    "                \"\"\"There one instance were an author spells his name both 'Demets' and 'DiMets'\"\"\"\n",
    "                if author[\"name\"] in [item for item in identifier_list.keys()]:\n",
    "\n",
    "                    # Update Values\n",
    "                    identifier_list[author[\"name\"]][\"weight\"] += 1\n",
    "                    identifier_list[author[\"name\"]][\"Years Active\"].append([pubmed_data[\"Date\"][i]])      \n",
    "\n",
    "                # If the author is not already in the dictionary\n",
    "                else:\n",
    "                    identifier_list[author[\"name\"]]  = {\"id\": id_count, \n",
    "                                                        \"Years Active\":[pubmed_data[\"Date\"][i]], \n",
    "                                                        \"affiliation\":[], \"weight\": 1, \"edges\":[]}\n",
    "\n",
    "                # Get year/affiliation data for GIS\n",
    "                if author[\"affiliation\"] != None:\n",
    "                    aff = author[\"affiliation\"]\n",
    "                    aff = re.sub(r\"[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+\", \"\", aff)  # Remove emails. Some author only list email.\n",
    "                    aff = re.sub(r\"(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)\", \"\", aff)  # Remove websites\n",
    "                    aff = aff.strip(\" \")\n",
    "                    if aff != \"\":  # For authors who only put an email as thier insitution\n",
    "                        identifier_list[author[\"name\"]][\"affiliation\"].append((aff, pubmed_data[\"Date\"][i]))\n",
    "\n",
    "                # Append other authors to edges list\n",
    "                temp_list = [o_author[\"name\"] for o_author in paper]\n",
    "                for item in temp_list:\n",
    "                    if item != author[\"name\"]:\n",
    "                        identifier_list[author[\"name\"]][\"edges\"].append(item) \n",
    "\n",
    "                id_count +=1\n",
    "            \n",
    "    with open(write_file, \"w\") as f:\n",
    "        json.dump(identifier_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"device_pubmed_data.json\", \"r\") as f:\n",
    "    pubmed_data = json.load(f)\n",
    "    get_author_info(pubmed_data=pubmed_data , write_file=\"device_author_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4902\n"
     ]
    }
   ],
   "source": [
    "with open(\"device_author_data.json\") as f:\n",
    "    test = json.load(f)\n",
    "    print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export location List\n",
    "with open(\"device_author_data.json\", \"r\") as f:\n",
    "    auth_list = json.load(f)\n",
    "\n",
    "# Need separte table for dates\n",
    "i = 1\n",
    "with open(\"./Statistical Data/institution_locations.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as csvfile: #In Arc: 21 addresses in the file could not be located.\n",
    "    writer = csv.writer(csvfile, delimiter=\",\")\n",
    "    writer.writerow([\"Index\", \"Location\", \"Year\"])  # Headers\n",
    "    for author in auth_list.values():\n",
    "        if author[\"affiliation\"] != []:\n",
    "            inst = author[\"affiliation\"]\n",
    "            for item in inst:\n",
    "                writer.writerow([i, item[0], eval(item[1]).year])\n",
    "                i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PubMed API Data Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
