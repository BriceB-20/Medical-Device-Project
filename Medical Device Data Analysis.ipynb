{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Work in Progress ***\n",
    "\n",
    "Pubmed Query Builder: https://pubmed.ncbi.nlm.nih.gov/advanced/\n",
    "\n",
    "### Notes\n",
    "* Dates stored in __repr__ format in JSON\n",
    "* 000 = Code for unobtainable PMUID\n",
    "\n",
    "### MeSH Terms\n",
    "* Biomedical Engineering \n",
    "* Biomedical Technology\n",
    "* Equipment Safety\n",
    "* Equipment Design\n",
    "* Prostheses and Implants\n",
    "\n",
    "### Querys\n",
    "((\"Equipment and Supplies\"[MeSH Terms] OR \"Equipment Design\"[MeSH Terms] OR \"Equipment Safety\"[MeSH Terms] OR \"Biomedical Technology\"[MeSH Terms] OR \"Biomedical Engineering\"[MeSH Terms]) AND 1850/01/01:1997/12/31[Date - Publication]) AND (English[Language])\n",
    "\n",
    "(((medical device[Title/Abstract]) OR (medical devices[Title/Abstract])) AND (english[Language])) AND ((\"1850\"[Date - Publication] : \"2003\"[Date - Publication]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-5Jn7QOhBFO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from collections import Counter  \n",
    "from matplotlib import pyplot as plt\n",
    "from pymed import PubMed\n",
    "import re\n",
    "import csv\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Implement date cutoff in query function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Pymed to get results for the desired query\n",
    "def search(query_string, pubmed_module, max_results=2000):\n",
    "    results = pubmed_module.query(query_string, max_results=max_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets relevant data from Pymed iterable and creates a dictionary\n",
    "def pubmed_todict(pubmed_iterable):\n",
    "    new_dict = {\"Result Number\": [], \"Pubmed ID\": [], \"Title\": [], \"Journal\": [], \"Authors\": [], \"Date\": []}\n",
    "    \n",
    "    # Assemble dictionary\n",
    "    for i, article in enumerate(pubmed_iterable):\n",
    "        new_dict[\"Result Number\"].append(i)\n",
    "        if \"\\n\" in article.pubmed_id:  # Deals with parsing issue for article PMUID\n",
    "            new_dict[\"Pubmed ID\"].append(\"000\")\n",
    "        else:\n",
    "            new_dict[\"Pubmed ID\"].append(article.pubmed_id)   \n",
    "        new_dict[\"Title\"].append(article.title)\n",
    "        try:  # Deals with books and book reviews (rather than articles) in the database\n",
    "            new_dict[\"Journal\"].append(article.journal)\n",
    "        except AttributeError:\n",
    "            new_dict[\"Journal\"].append(\"Book\") \n",
    "        new_dict[\"Authors\"].append(article.authors)\n",
    "        if type(article.publication_date) == datetime.date:  # Standardizes data - pymed returns some years as int\n",
    "            new_dict[\"Date\"].append(article.publication_date) \n",
    "        else:\n",
    "            new_dict[\"Date\"].append(datetime.datetime.strptime(str(article.publication_date), \"%Y\"))\n",
    "        \n",
    "    # Cleans author data\n",
    "    clean_author_list_by_paper = []\n",
    "    for paper_author_list in new_dict[\"Authors\"]:\n",
    "        if paper_author_list == []:\n",
    "            clean_author_list_by_paper.append([{\"name\": None, \"affiliation\": None}])\n",
    "        else:\n",
    "            author_list_by_paper = []\n",
    "            for author in paper_author_list:\n",
    "                if author[\"lastname\"] != None or author[\"initials\"] != None:\n",
    "                    try:\n",
    "                        author_dict = {\"name\": \"{} {}\".format(author[\"lastname\"], author[\"initials\"]),\n",
    "                                \"affiliation\": author[\"affiliation\"]}\n",
    "                    except KeyError:\n",
    "                        author_dict = {\"name\": \"{} {}\".format(author[\"lastname\"], author[\"initials\"]),\n",
    "                                    \"affiliation\": None}\n",
    "                    author_list_by_paper.append(author_dict)\n",
    "            clean_author_list_by_paper.append(author_list_by_paper)\n",
    "    \n",
    "    new_dict[\"Authors\"] = clean_author_list_by_paper\n",
    "    \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoids serialization error with datetime in JSON dump\n",
    "# See https://stackoverflow.com/questions/54557568/typeerror-object-of-type-date-is-not-json-serializable\n",
    "def datetime_converter(object):\n",
    "    if isinstance(object, datetime.date):\n",
    "        return object.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_dump(query_string, max_results, file_name):\n",
    "    pubmed = PubMed(tool=\"Medical Device Author Network Analysis\", email=\"bowrey@umd.edu\")\n",
    "    results = search(query_string=query_string, pubmed_module=pubmed, max_results=max_results)\n",
    "    new_dict = pubmed_todict(pubmed_iterable=results)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        # Default argument takes function that is called when JSON ecounters object it can't convert\n",
    "        json.dump(new_dict, f, indent=4, default=datetime_converter)\n",
    "        print(\"Search Completed. File dumped to JSON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searches for all items with \"medical device(s)\" in the title or abstract\n",
    "search_and_dump(query_string='''(((medical device[Title/Abstract]) OR (medical devices[Title/Abstract])) AND (english[Language])) \n",
    "                                AND ((\"1850\"[Date - Publication] : \"2004\"[Date - Publication]))''',\n",
    "               max_results=3000, file_name=\"device_pubmed_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publication Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some articles are listed with abnormal years (e.g. years outside of the range that was input to the API). I believe this is due to the Pymed module pulling the date of an article cited by the article in question rather than the date for the article itself. These errors are not randomly distributed. They overwhelmingly happen in the final year of the search range. My current solution is to extend the search range beyond what I want (up to the year 2000), then discarding all values outside of that range. This should pick up most of the incorrectly labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(file_path, cutoff_year=2000):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        full_dict = json.load(f)\n",
    "    \n",
    "    slim_dict = {key:value for (key, value) in full_dict.items() if key in [\"Pubmed ID\", \"Title\", \"Journal\", \"Date\"]}\n",
    "    df = pd.DataFrame.from_dict(slim_dict)\n",
    "    # df = df[df[\"Pubmed ID\"] != \"000\"]  # Removes data with parsing error\n",
    "    df[\"Date\"] = df[\"Date\"].apply(lambda x : eval(x).year)  # Converts date from __repr__ format\n",
    "    \n",
    "    # Cleaning years\n",
    "    for index in df.loc[df[\"Date\"] > cutoff_year].index:\n",
    "        df.drop([index], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_df = create_df(\"device_pubmed_data.json\", 2000)\n",
    "device_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publications by Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_date_group = device_df.groupby([\"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_pub_year_table = device_date_group[\"Journal\"].apply(lambda y: y.agg(lambda x : x.value_counts().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(device_pub_year_table.index, [device_pub_year_table.get(x) for x in device_pub_year_table.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Export to Voyant Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voyant_export(year_group, dataset_name, start_year, end_year=2000):\n",
    "    year_title_list = year_group[\"Title\"].unique()\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        temp_list = []\n",
    "        try:\n",
    "            for title in year_title_list[year]:\n",
    "                title = title.replace(\"--\", \" \")\n",
    "                title = title + \"\\n\"\n",
    "                temp_list.append(title)\n",
    "            \n",
    "            with open(\"Title Data Files/med_dev/{}_{}.txt\".format(year, dataset_name), \"w\") as f:\n",
    "                f.writelines(temp_list)\n",
    "       \n",
    "        except KeyError:  # If there were no publication in a year in the range\n",
    "            pass\n",
    "    \n",
    "    print(\"Data Exported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical Device Publication Word Frequency: https://voyant-tools.org/?corpus=fa33b04ff82a6bb8eb2bb7c2b5c9837a&panels=corpusterms,termsberry,trends,summary,contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voyant_export(year_group = device_date_group, dataset_name=\"med_dev\", start_year=1952, end_year=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create author info json file\n",
    "def get_author_info(pubmed_data, write_file):\n",
    "    identifier_list = {}\n",
    "    id_count = 1\n",
    "    \n",
    "    # Create data structure for author\n",
    "    for i, paper in enumerate(pubmed_data[\"Authors\"]):\n",
    "        for author in paper:\n",
    "            if author[\"name\"] != None:\n",
    "                \n",
    "                # If author is already in the dictionary\n",
    "                \"\"\"There one instance were an author spells his name both 'Demets' and 'DiMets'\"\"\"\n",
    "                if author[\"name\"] in [item for item in identifier_list.keys()]:\n",
    "\n",
    "                    # Update Values\n",
    "                    identifier_list[author[\"name\"]][\"weight\"] += 1\n",
    "                    identifier_list[author[\"name\"]][\"years active\"].append(pubmed_data[\"Date\"][i])      \n",
    "\n",
    "                # If the author is not already in the dictionary\n",
    "                else:\n",
    "                    identifier_list[author[\"name\"]]  = {\"id\": id_count, \n",
    "                                                        \"years active\":[pubmed_data[\"Date\"][i]], \n",
    "                                                        \"affiliation\":[], \"weight\": 1, \"edges\":[]}\n",
    "\n",
    "                # Get year/affiliation data for GIS\n",
    "                if author[\"affiliation\"] != None:\n",
    "                    aff = author[\"affiliation\"]\n",
    "                    aff = re.sub(r\"[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+\", \"\", aff)  # Remove emails. Some author only list email.\n",
    "                    aff = re.sub(r\"(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)\", \"\", aff)  # Remove websites\n",
    "                    aff = aff.strip(\" \")\n",
    "                    if aff != \"\":  # For authors who only put an email as thier insitution\n",
    "                        identifier_list[author[\"name\"]][\"affiliation\"].append((aff, pubmed_data[\"Date\"][i]))\n",
    "\n",
    "                # Append other authors to edges list\n",
    "                temp_list = [(o_author[\"name\"], pubmed_data[\"Date\"][i]) for o_author in paper]  # Co-author name and year of collaboration\n",
    "                for item in temp_list:\n",
    "                    if item[0] != author[\"name\"]:\n",
    "                        identifier_list[author[\"name\"]][\"edges\"].append(item) \n",
    "\n",
    "                id_count +=1\n",
    "            \n",
    "    with open(write_file, \"w\") as f:\n",
    "        json.dump(identifier_list, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"device_pubmed_data.json\", \"r\") as f:\n",
    "    pubmed_data = json.load(f)\n",
    "    get_author_info(pubmed_data=pubmed_data , write_file=\"device_author_data.json\")"
   ]
  },
  {
   "source": [
    "## Neo4J"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"device_author_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    # author_list = [key for key in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"devices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_author_nodes(file_path):\n",
    "#     def add_author(tx, name, author_data=author_data):\n",
    "#         if len(author_data) > 0:\n",
    "#             c_dict = Counter(author_data[name][\"edges\"])\n",
    "#             for coauthor in author_data[name][\"edges\"]:\n",
    "#                 tx.run(\"MERGE (a: author {id: $id, name: $name, years_active: $years_active}) \\\n",
    "#                         -[:COAUTHOR {weight: weight}]- \\\n",
    "#                         (b: author {name: $coauthor_name}) \\\n",
    "#                         ON CREATE SET \\\n",
    "#                         b.id=$c_id, b.years_active=$c_years_active\",\n",
    "#                         id=author_data[name][\"id\"], name=name, \n",
    "#                         years_active= author_data[name][\"years active\"],\n",
    "#                         weight=c_dict[coauthor], coauthor_name=coauthor, \n",
    "#                         c_id=author_data[coauthor][\"id\"], \n",
    "#                         c_years_active=author_data[coauthor][\"years active\"])\n",
    "\n",
    "#     with open(file_path, \"r\") as f:\n",
    "#         author_data = json.load(f)\n",
    "\n",
    "\n",
    "#     with driver.session() as session:\n",
    "#         for author in author_data:\n",
    "#             tx.run(\"MERGE (a: author {id: $id, name: $name, years_active: $years_active})\", \n",
    "#     name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_author_node(tx, author_name, author_data):\n",
    "        tx.run(\"CREATE (a: author {id: $id, name: $name, years_active: $years_active})\", \n",
    "        id=author_data[author_name][\"id\"], name=author_name, \n",
    "        # Some authors have multiple papers published in the same year. Ex: Munzner RF\n",
    "        years_active=[eval(date).year for date in author_data[author_name][\"years active\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_relationships(tx, author_name, author_data):\n",
    "    for coauthor in author_data[author_name][\"edges\"]:\n",
    "        tx.run(\"\"\"MATCH (a: author {id: $a_id, name: $a_name})\n",
    "        MATCH (b: author {id: $b_id, name: $b_name})\n",
    "        CREATE (a)-[c:COAUTHOR {date: $date}]->(b)\"\"\",\n",
    "        a_id=author_data[author_name][\"id\"],\n",
    "        a_name=author_name,\n",
    "        b_id=author_data[coauthor[0]][\"id\"],\n",
    "        b_name=coauthor[0],\n",
    "        date=eval(coauthor[1]).isoformat())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"device_author_data.json\", \"r\") as f:\n",
    "        author_data = json.load(f)\n",
    "\n",
    "# with driver.session() as session:\n",
    "#     for author_name in author_data.keys():\n",
    "#         session.write_transaction(create_author_node, author_name, author_data)\n",
    "\n",
    "with driver.session() as session:\n",
    "    for author_name in author_data.keys():\n",
    "        session.write_transaction(add_relationships, author_name, author_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build insitutions dataframe\n",
    "with open(\"device_author_data.json\", \"r\") as f:\n",
    "    auth_list = json.load(f)\n",
    "\n",
    "location_dict = {\"ID\": [], \"Location\": [], \"Year\": []}\n",
    "\n",
    "i = 1\n",
    "for author in auth_list.values():\n",
    "    if author[\"affiliation\"] != []:\n",
    "        inst = author[\"affiliation\"]\n",
    "        for item in inst:\n",
    "            location_dict[\"ID\"].append(i)\n",
    "            location_dict[\"Location\"].append(item[0])\n",
    "            location_dict[\"Year\"].append(eval(item[1]))\n",
    "            i +=1\n",
    "\n",
    "loc_df = pd.DataFrame.from_dict(location_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.to_csv(r\"./GIS/institution_years.csv\", columns=[\"Year\"], index_label=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df[[\"ID\", \"Year\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export unique location List\n",
    "un_loc_df = loc_df.drop_duplicates(\"Location\", ignore_index=True)\n",
    "#loc_df.to_csv(\"Medical-Device-Project/GIS/institution_locations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "PubMed API Data Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}